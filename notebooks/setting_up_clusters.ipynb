{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6320d2b-8c56-4fd0-9d4e-b6d088431190",
   "metadata": {},
   "source": [
    "# Deploying Dask clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0401c1-d637-4b19-9310-4120f0290ed5",
   "metadata": {},
   "source": [
    "## 1) local machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe103f-2440-484d-9235-c700ac11e158",
   "metadata": {},
   "source": [
    "### 1.1) Threads on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9636632-0ee8-4229-ac16-208bdbe58c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b64d33-ddf1-4c9d-9944-2e0a3abb4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stuff directly\n",
    "# df = dd.read_csv(...)\n",
    "# df.x.sum().compute()  # This uses threads on your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e4baa-846e-4946-8a45-4ab4681b94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitely configure Dask to use multi-threading\n",
    "dask.config.set(scheduler='threads')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1abd3-5480-470d-adfc-a38d7fdd9f92",
   "metadata": {},
   "source": [
    "### 1.2) Multiprocessing on local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc0399-cd4e-4dd2-bc6d-524210493138",
   "metadata": {},
   "source": [
    "The multi-process backend is able to avoid Python’s global interpreter lock by launching separate processes. Launching a new process is **more expensive than a new thread**, and Dask needs to serialize data that moves between processes.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc7c3f1c-5eb2-4019-822b-3b31dcc902af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsoler/.cache/pypoetry/virtualenvs/daskscratch-1ORGWtuD-py3.12/lib/python3.12/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 37643 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "cluster = LocalCluster()          # Fully-featured local Dask cluster\n",
    "client = cluster.get_client()\n",
    "\n",
    "# Dask works as normal and leverages the infrastructure defined above\n",
    "# df.x.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2928be9-09f1-495c-b23e-d96c2e243612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally, you can configure dask to use multiprocessing by default on your local backend\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# Using the forkserver will not reduce the communication overhead.\n",
    "dask.config.set({\"multiprocessing.context\": \"forkserver\",\n",
    "                \"scheduler\": \"processes\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65446387-0a61-4151-93a4-0cb369404ec0",
   "metadata": {},
   "source": [
    "The LocalCluster cluster manager defined above is easy to use and works well on a single machine. It follows the **same interface** as all other Dask cluster managers, and so it’s easy to swap out when you’re ready to scale up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e155eb-b638-4dc3-b6ab-a3fc5104a65a",
   "metadata": {},
   "source": [
    "## 2) Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77dcd0a-90a8-4a5a-8cf3-5216bddd7992",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask_kubernetes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask_kubernetes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KubeCluster\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# cluster = LocalCluster()\u001b[39;00m\n\u001b[32m      4\u001b[39m cluster = KubeCluster()  \u001b[38;5;66;03m# example, you can swap out for Kubernetes\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dask_kubernetes'"
     ]
    }
   ],
   "source": [
    "from dask_kubernetes import KubeCluster\n",
    "\n",
    "# cluster = LocalCluster()\n",
    "cluster = KubeCluster()  # example, you can swap out for Kubernetes\n",
    "\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d534c-f38c-4ea4-8e71-6adae6b6f4df",
   "metadata": {},
   "source": [
    "## 3) Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe40f10-97e4-4ded-9c27-270ef99d14e8",
   "metadata": {},
   "source": [
    "Deploying on commercial cloud like AWS, GCP, or Azure is convenient because you can quickly scale out to many machines for just a few minutes, but also challenging because you need to navigate awkward cloud APIs, manage remote software environments with Docker, send data access credentials, make sure that costly resources are cleaned up, etc. The following solutions help with this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "360b1d33-4769-4725-acff-96a573925af5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'coiled'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Coiled (recommended): this commercial SaaS product handles most of the deployment pain Dask users encounter, \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# is easy to use, and quite robust. The free tier is large enough for most individual users, even for those who don’t want to engage with a commercial company. The API looks like the following.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcoiled\u001b[39;00m\n\u001b[32m      5\u001b[39m cluster = coiled.Cluster(\n\u001b[32m      6\u001b[39m     n_workers=\u001b[32m100\u001b[39m,\n\u001b[32m      7\u001b[39m     region=\u001b[33m\"\u001b[39m\u001b[33mus-east-2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     worker_memory=\u001b[33m\"\u001b[39m\u001b[33m16 GiB\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     spot_policy=\u001b[33m\"\u001b[39m\u001b[33mspot_with_fallback\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m client = cluster.get_client()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'coiled'"
     ]
    }
   ],
   "source": [
    "# Coiled (recommended): this commercial SaaS product handles most of the deployment pain Dask users encounter, \n",
    "# is easy to use, and quite robust. The free tier is large enough for most individual users, even for those who don’t want to engage with a commercial company. The API looks like the following.\n",
    "\n",
    "import coiled\n",
    "cluster = coiled.Cluster(\n",
    "    n_workers=100,\n",
    "    region=\"us-east-2\",\n",
    "    worker_memory=\"16 GiB\",\n",
    "    spot_policy=\"spot_with_fallback\",\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "# Dask Cloud Provider: a pure and simple OSS solution that sets up Dask workers on cloud VMs, supporting AWS, GCP, Azure, \n",
    "# and also other commercial clouds like Hetzner, Digital Ocean and Nebius.\n",
    "\n",
    "# Dask-Yarn: deploys Dask on legacy YARN clusters, such as can be set up with AWS EMR or Google Cloud Dataproc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294518e-4f7a-4b01-b335-1596dd6446f0",
   "metadata": {},
   "source": [
    "## 4) HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de369941-3b47-4d6d-83a8-b0ca6968d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask runs on traditional HPC systems that use a resource manager like SLURM, PBS, SGE, LSF, or similar systems,\n",
    "# and a network file system. This is an easy way to dual-purpose large-scale hardware for analytics use cases. \n",
    "# Dask can deploy either directly through the resource manager or through mpirun/mpiexec and \n",
    "# tends to use the NFS to distribute data and software.\n",
    "\n",
    "# Dask-Jobqueue (recommended): interfaces directly with the resource manager (SLURM, PBS, SGE, LSF, and others)\n",
    "# to launch many Dask workers as batch jobs. It generates batch job scripts and submits them automatically to the user’s queue.\n",
    "# This approach operates entirely with user permissions (no IT support required) and enables interactive and adaptive use on large HPC systems.\n",
    "# It looks a little like the following:\n",
    "\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "cluster = PBSCluster(cores=36,\n",
    "                     memory=\"100GB\",\n",
    "                     project='P48500028',\n",
    "                     queue='premium',\n",
    "                     interface='ib0',\n",
    "                     walltime='02:00:00')\n",
    "\n",
    "cluster.scale(100)  # Start 100 workers in 100 jobs that match the description above\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)    # Connect to that cluster\n",
    "\n",
    "# Dask-MPI: deploys Dask on top of any system that supports MPI using mpirun. It is helpful for batch processing jobs where you want to ensure a fixed and stable number of workers.\n",
    "\n",
    "# Dask Gateway for Jobqueue: Multi-tenant, secure clusters. Once configured, users can launch clusters without direct access to the underlying HPC backend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1310b-f586-4cda-9239-3a0ea21e3cb4",
   "metadata": {},
   "source": [
    "More details for HPC on [this page](https://docs.dask.org/en/stable/deploying-hpc.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69568cb-3a9a-4ed1-9607-d7541ba5344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use dask_mpi (better suited for batch jobs)\n",
    "\n",
    "# set up a scheduler\n",
    "mpirun --np 4 dask-mpi --scheduler-file /home/$USER/scheduler.json\n",
    "\n",
    "# start the cluster, providing the scheduler address\n",
    "from dask.distributed import Client\n",
    "client = Client(scheduler_file='/path/to/scheduler.json')\n",
    "\n",
    "# This depends on the mpi4py library. It only uses MPI to start the Dask cluster and not for inter-node communication. \n",
    "# MPI implementations differ: the use of mpirun --np 4 is specific to the mpich or open-mpi MPI implementation installed through \n",
    "# conda and linked to mpi4py.\n",
    "\n",
    "# conda install mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aa917a-8083-4d6c-96c4-003e0dfec1a5",
   "metadata": {},
   "source": [
    "## 5) Dask gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b44f5-7961-42ac-8d92-d13f0c70119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask Gateway provides a secure, multi-tenant server for managing Dask clusters.\n",
    "# It allows users to launch and use Dask clusters in a shared, centrally managed cluster environment,\n",
    "# without requiring users to have direct access to the underlying cluster backend (e.g. Kubernetes, Hadoop/YARN, HPC Job queues, etc…).\n",
    "\n",
    "# helm install --repo https://helm.dask.org --create-namespace -n dask-gateway --generate-name dask-gateway\n",
    "\n",
    "from dask_gateway import Gateway\n",
    "gateway = Gateway(\"<gateway service address>\")\n",
    "cluster = gateway.new_cluster()\n",
    "\n",
    "# This is a good choice if you want to do the following:\n",
    "#  - Abstract users away from Kubernetes.\n",
    "\n",
    "# - Provide a consistent Dask user experience across Kubernetes/Hadoop/HPC.\n",
    "\n",
    "# Learn more at gateway.dask.org."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ec2ff-bdf1-4ef0-8665-602854636605",
   "metadata": {},
   "source": [
    "## 6) Command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d851a4-c84f-4982-8220-e7832b5e89c1",
   "metadata": {},
   "source": [
    "See https://docs.dask.org/en/stable/deploying-cli.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d1c04a-3e19-4e05-ae29-df30ae1a8d93",
   "metadata": {},
   "source": [
    "## 7) Autoscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de724d-70f8-42d4-8e42-0bcb49b5e351",
   "metadata": {},
   "source": [
    "\n",
    "Auto-scaling\n",
    "\n",
    "With auto-scaling, Dask can increase or decrease the computers/resources being used, based on the tasks you have asked it to run. For example, if you have a program that computes complex aggregations using many computers but then mostly operates on the aggregated data, the number of computers you need could decrease by a large amount post-aggregation. Many workloads, including machine learning, do not need the same amount of resources/computers the entire time.\n",
    "\n",
    "Some of Dask’s cluster backends, including Kubernetes, support auto-scaling, which Dask calls adaptive deployments. Auto-scaling is useful mostly in situations of shared cluster resources, or when running on cloud providers where the underlying resources are paid for by the hour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8273e9d7-ebe5-4c5e-81b7-bf7eda0dab52",
   "metadata": {},
   "source": [
    "## 8) data serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed1740-a8a9-4fe5-8d7f-51fa03598c39",
   "metadata": {},
   "source": [
    "Part of why Dask is so powerful is the Python ecosystem that it is in. While Dask will pickle, or serialize (see “Serialization and Pickling”), and send our code to the workers, this doesn’t include the libraries we use.6 To take advantage of that ecosystem, you need to be able to use additional libraries. During the exploration phase, it is common to install packages at runtime as you discover that you need them.\n",
    "\n",
    "The PipInstall worker plug-in takes a list of packages and installs them at runtime on all of the workers. Looking back at Example 2-4, to install bs4 you would call distributed.diagnostics.plugin.PipInstall([\"bs4\"]). Any new workers that are launched by Dask then need to wait for the package to be installed. The Pip​Install plug-in is ideal for quick prototyping when you are discovering which packages you need. You can think of PipInstall as the replacement for !pip install in a notebook over having a virtualenv.\n",
    "\n",
    "To avoid the slowness of having to install packages each time a new worker is launched, you should try to pre-install your libraries. Each cluster manager (e.g., YARN, Kubernetes, Coiled, Saturn, etc.) has its own methods for managing dependencies. This can happen at runtime or at setup where the packages are pre-installed. The specifics for the different cluster managers are covered in Chapter 12.\n",
    "\n",
    "With Kubernetes, for example, the default startup script checks for the presence of some key environment variables (EXTRA_APT_PACKAGES, EXTRA_CONDA_PACKAGES, and EXTRA_PIP_PACKAGES), which, in conjunction with customized worker specs, can be used to add dependencies at runtime. Some of them, like Coiled and Kubernetes, allow for adding dependencies when building an image for our workers. Others, like YARN, use preallocated conda/virtual environment packing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
